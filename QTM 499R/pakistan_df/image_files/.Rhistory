# (b) Alejandro's PC
rm(list = ls())
if(Sys.info()["user"] == "asanc31") {
dirhat     <- "" # TO BE DEFINED
} else if(Sys.getenv("USERNAME")=="asanc"){
dirhat  <- paste0("D:/Dropbox/W_PeerEffects/")
}
# Secondary folders
datadir <- paste0(dirhat,"Data/CleanData_2023/")
1700000*0.06
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.
#------------------------------------------------------------------------------#
# Process expert list -- first round
# Author: Alejandro Sanchez, Joel Becker
# Notes:
#
#------------------------------------------------------------------------------#
########################################################
######################## Set-up ########################
########################################################
# clear wd
rm(list = ls())
# load libraries
packages <- c(
"dplyr",        # Manage data cleaning
"haven",            # Imports Stata database
"readxl",           # Import excel files
"splitstackshape",  # Clean columns with lists (e.g. authors, categories,etc)
"bibtex",
"bib2df",
"rworldmap",
"stringr",          # Special operations with strings
"estimatr",
"openxlsx",
"tidyr",
"tidyverse",
"readr",
"plyr",
"feather", # Light data files
"janitor", # Tidy tables
"ggplot2"
)
new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(packages, library, character.only = TRUE)
main_dir           <- "D:/GitHub/expert-forecasting"
raw_dir <- paste0(main_dir, "/raw_data/3_get_policymaker_list/ngo_india_scraping")
stored_dir <- paste0(main_dir, "/output/3_get_policymaker_list/")
list_rawfiles_ngo <- list.files(path=raw_dir, pattern="*.csv", full.names=TRUE)
# scraped_data <- ldply(list_rawfiles_ngo, read_csv,show_col_types = FALSE)
# write_csv(scraped_data,paste0(stored_dir,"scraped_data_ngos_india.csv"))
scraped_data <- as_tibble(read_csv(paste0(stored_dir,"/scraped_data_ngos_india.csv")))
names(scraped_data)
# Check for e-mail information
mean(!is.na(scraped_data$contact_details_email))
mean(1*(scraped_data$contact_details_email != "Not Available"),na.rm = TRUE)
# Check for website information
mean(!is.na(scraped_data$contact_details_website))
mean(1*(scraped_data$contact_details_website != "Not Available"),na.rm = TRUE)
# Clean data
scraped_data <- mutate_if(scraped_data,is.character,
stringr::str_replace_all, pattern = "&amp;",
replacement = "&")
#--------------------------- SUMMARY STATS FOR FULL DATASET -------------------#
# Count NGOs scraped by state
# This is useful as a quality diagnostic because sometimes the scraping
# algorithm does not complete the data extraction due to interrupted connections.
table_countngos_state  <- as_tibble(scraped_data %>%
tabyl(sector_number, page_num)) %>%
select("sector_number",paste0(1:194))
# Check a specific page (for diagnostic)
check_page <- scraped_data %>%
filter(state_of_registration=="ANDHRA PRADESH",
page_num == 12)
# Count NGOs by state
table_state <- tibble(statefreq = table(scraped_data$sector_number))
data_intermediate <-scraped_data %>%
tabyl(state_of_registration, page_num)
sum(data_intermediate[10,-1] %>% as.matrix())
#--------------------------- SUBSET DATA BY ISSUES ----------------------------#
# Obtain list of key issues
key_issues_list <- unique(cSplit(scraped_data,
splitCols=c("key_issues"),
sep =",", direction="long",
type.convert = FALSE) %>% select(key_issues))
subset_keyissues <- c("Micro Finance (SHGs)",
"Rural Development & Poverty Alleviation",
"Urban Development & Poverty Alleviation",
"Labour & Employment",
"Women's Development & Empowerment",
"Micro Small & Medium Enterprise")
# Draw a subset of maybes:
# "Health & Family Welfare",
# "Vocational Training",
adjacent_keyissues <- c("Micro Small & Medium Enterprise",
"Information & Communication Technology",
"Nutrition",
"Skill Development")
excluded_keyissues <- setdiff(unlist(key_issues_list), c(subset_keyissues,
adjacent_keyissues))
excluded_keyissues <- c("Sports","Not Available","Art & Culture",
"Aged/Elderly","Differently Abled",
"Drinking Water","Environmental & Forests",
"HIV/AIDS","Housing",
"Micro Small & Medium Enterprises",
"Tourism",
"Animal Husbandry",
"Dairying & Fisheries",
"Biotechnology",
"Prisoner's Issues",
"New & Renewable Energy",
"Tribal Affairs",
"Water Resources")
subset_ngos <- scraped_data %>%
filter(grepl(paste0(subset_keyissues,collapse = "|"), key_issues)) %>%
filter(!grepl(paste0(excluded_keyissues,collapse = "|"), key_issues))
subset_ngos
#------------------------- Compute Descriptive Statistics ---------------------#
# When were NGOs founded?
hist(as.Date(subset_ngos$registration_date,format = c("%d-%m-%Y")),
breaks = 50)
# What states do the NGOs belong to
ggplot(subset_ngos,aes(x=state_of_registration)) +
geom_bar() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
#ggplot(scraped_data,aes(x=state_of_registration)) +
#  geom_bar() +
#  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# How many of them list a website
mean(1*(subset_ngos$contact_details_website != "Not Available"),na.rm = TRUE)
table(subset_ngos$type_of_ngo)
write_csv(subset_ngos,paste0(stored_dir,"subset_ngos_india.csv"))
#------------------------- Compute Descriptive Statistics ---------------------#
# When were NGOs founded?
hist(as.Date(subset_ngos$registration_date,format = c("%d-%m-%Y")),
breaks = 50)
# What states do the NGOs belong to
ggplot(subset_ngos,aes(x=state_of_registration)) +
geom_bar() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# How many of them list a website
mean(1*(subset_ngos$contact_details_website != "Not Available"),na.rm = TRUE)
table(subset_ngos$type_of_ngo)
data_intermediate
pnorm(0.7)-pnorm(0.2)
install.packages("tesseract")
library("tesseract")
install.packages("pdftools")
install.packages("tesseract")
install.packages("devtools")
install.pacakages("tesseract")
install.packages("tesseract")
library("pdftools")
install.packages("tesseract")
library("tesseract")
setwd("D:\OneDrive - Emory University\QTM 499R\ngo_raw_lists\pakings_listngos")
setwd("D:/OneDrive - Emory University/QTM 499R/ngo_raw_lists/pakings_listngos")
# Did not set seed prior to writing the csv so results will differ, but seed
# will be set to 499 henceforth.
set.seed(499)
rawdata_onedrive_pk <- paste0(onedrive, "/pakistan_df")
# Directory to QTM 499R Folder in OneDrive
onedrive         <- "/Users/jspgr33n/Library/CloudStorage/OneDrive-EmoryUniversity/QTM-Research/QTM 499R"
rawdata_onedrive_pk <- paste0(onedrive, "/pakistan_df")
ngobase_validity_websites <- read.csv(paste0(rawdata_onedrive_pk,"/ngobase_validity_websites.csv"))
ngobase_validity_websites
rawdata_onedrive_bg <- paste0(onedrive, "/bangladesh_df/new_files")
View(ngobase_validity_websites)
ngobase_validity_websites %>%
filter(website_found == "Yes")
#---------------------------------------------------------------------------#
library(tesseract)
library(pdftools)
library(tidyverse)
library(dplyr)
library(stringr)
library(ggplot2)
library(lubridate)
ngobase_validity_websites %>%
filter(website_found == "Yes")
ngobase_validity_websites %>%
filter(website_found == "Yes")
website_found_count <- ngobase_validity_websites %>%
filter(website_found == "Yes")
website_found_count <- ngobase_validity_websites %>%
filter(website_found == "Yes") %>%
nrow()
website_found_count
website_found_count/nrow(ngobase_validity_websites)
ngobase_validity
ngobase_validity
rawdata_onedrive_pk <- paste0(onedrive, "/pakistan_df")
#---------------------------------------------------------------------------#
#-------------------------SET UP WORKING ENVIRONMENT------------------------#
#---------------------------------------------------------------------------#
library(tesseract)
library(pdftools)
library(tidyverse)
library(dplyr)
library(stringr)
library(ggplot2)
library(lubridate)
# Directory to QTM 499R Folder in OneDrive
onedrive         <- "/Users/jspgr33n/Library/CloudStorage/OneDrive-EmoryUniversity/QTM-Research/QTM 499R"
rawdata_onedrive_pk <- paste0(onedrive, "/pakistan_df")
#---------------------------------------------------------------------------#
#-------------------50 RANDOM PAKISTAN NGOs FOR NGOBASE---------------------#
#---------------------------------------------------------------------------#
# NGOBase has 147 pages with around 10 NGOs on each page.
page_number <- sample(1:147,50, replace = TRUE)
list_number <- sample(1:10,50, replace = TRUE)
# Did not set seed prior to writing the csv so results will differ, but seed
# will be set to 499 henceforth.
set.seed(499)
ngobase_validity <- data.frame(page_number, list_number)
write.csv(ngobase_validity, paste0(rawdata_onedrive_pk,"/ngobase_validity.csv"))
ngobase_validity_websites <- read.csv(paste0(rawdata_onedrive_pk,"/ngobase_validity_websites.csv"))
# Count how many NGOs have websites
website_found_count <- ngobase_validity_websites %>%
filter(website_found == "Yes") %>%
nrow()
# Calculate percentage of websites found given 50 random NGOs
website_found_count/nrow(ngobase_validity_websites)
#---------------------------------------------------------------------------#
#-----------------50 RANDOM PDF NGOs OBSERVED ON NGOBASE--------------------#
#---------------------------------------------------------------------------#
# The pakistan_listngos pdf file has 474 NGOs. Select 50 random samples and
# determine whether or not if they can be found on NGOBase
pakistan_listngos <- sample(1:474, 50, replace = FALSE)
pakistan_listngos
ngobase_validity
pakistan_listngos <- data.frame(pakistan_listngos)
pakistan_listngos
pakistan_listngos_ngobase <- data.frame(pakistan_listngos)
pakistan_listngos <- sample(1:474, 50, replace = FALSE)
pakistan_listngos_ngobase <- data.frame(pakistan_listngos)
write.csv(pakistan_listngos_ngobase, paste0(rawdata_onedrive_pk,"/pakistan_listngos_ngobase.csv"))
# Clean working environment
rm(list = ls())
# Import packages
library(tesseract)
library(magick)
library(magrittr)
library(tidyverse)
library(readxl)
if(Sys.info()["user"] == "jspgr33n") {
dirhat     <- paste0("/Users/jspgr33n/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM-Research/QTM 499R/pakistan_df/")
} else if(Sys.getenv("USERNAME")=="jspgr33n"){
dirhat <- ""
}
# Set image directory
imagedir <- paste0(dirhat,"image_files")
setwd(imagedir) # This is where the PNG files will be stored.
# Define name of PDF
filename <- "pakistan_listngos_Website3.pdf"
# Last updated: February 24, 2024
# INSTRUCTIONS FOR JAMES:
# This document extracts data from a PDF of pakistan NGOs.
# It requires TWO manual inputs: (1) A parameter to rotate the PDF.
#                                (2) The coordinates of the table.
# Since each page in the PDF has a slightly different rotation and position
# then we have tweak this manually for each.
# Once we enter this information manually, I (Alejandro) have coded up
# an algorithm that automatically extracts the information.
#
# STRENGTHS: The algorithm seems to extract the names of organizations perfectly
#            now, and their respective numbers.
# WEAKNESSES: The MOU signing date is computed, but there are still some issues.
#           Overall, the code is very sensitive to how we define the coordinates.
#           Please pay particular attention to how I defined the coordinates for
#           pages 1-2 and then do the same for pages 3-10.
####.
###
## WHAT TO FOCUS ON? SECTION III.
###
#
##
#
#------------------------------------------------------------------------------#
#------------------- I. SET UP WORKING DIRECTORY ------------------------------#
#------------------------------------------------------------------------------#
# Clean working environment
rm(list = ls())
# Import packages
library(tesseract)
library(magick)
library(magrittr)
library(tidyverse)
library(readxl)
# eng <- tesseract("eng")
# Set directory (automatically detecting computer)
# The first time you will need to enter this manually
# -- Changed from Dr. Sanchez-Becerra's directory to James' directory
if(Sys.info()["user"] == "jspgr33n") {
dirhat     <- paste0("/Users/jspgr33n/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM-Research/QTM 499R/pakistan_df/")
} else if(Sys.getenv("USERNAME")=="jspgr33n"){
dirhat <- ""
} else if(Sys.info()["user"] == "asanc31"){
dirhat     <- paste0("/Users/jspgr33n/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM-Research/QTM 499R/pakistan_df/")
}
if(Sys.info()["user"] == "jspgr33n") {
dirhat     <- paste0("/Users/jspgr33n/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM-Research/QTM 499R/pakistan_df/")
} else if(Sys.getenv("USERNAME")=="jspgr33n"){
dirhat <- ""
} else if(Sys.info()["user"] == "asanc31"){
dirhat     <- paste0("/Users/asanc31/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM-Research/QTM 499R/pakistan_df/")
}
# Set image directory
imagedir <- paste0(dirhat,"image_files")
setwd(imagedir) # This is where the PNG files will be stored.
dirhat
# Set image directory
imagedir <- paste0(dirhat,"image_files")
setwd(imagedir) # This is where the PNG files will be stored.
dirhat
# Set image directory
imagedir <- paste0(dirhat,"image_files")
setwd(imagedir) # This is where the PNG files will be stored.
# Set image directory
imagedir <- paste0(dirhat,"image_files/")
setwd(imagedir) # This is where the PNG files will be stored.
imagedir
# Set image directory
imagedir <- paste0(dirhat,"image_files")
imagedir
dirhat     <- paste0("/Users/asanc31/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM 499R/pakistan_df/")
if(Sys.info()["user"] == "jspgr33n") {
dirhat     <- paste0("/Users/jspgr33n/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM-Research/QTM 499R/pakistan_df/")
} else if(Sys.getenv("USERNAME")=="jspgr33n"){
dirhat <- ""
} else if(Sys.info()["user"] == "asanc31"){
dirhat     <- paste0("/Users/asanc31/Library/CloudStorage/",
"OneDrive-EmoryUniversity/QTM 499R/pakistan_df/")
}
# Set image directory
imagedir <- paste0(dirhat,"image_files")
setwd(imagedir) # This is where the PNG files will be stored.
# Define name of PDF
filename <- "pakistan_listngos_Website3.pdf"
# Convert PDF to separate PNG and store in "imagedir"
pngfile_list <-
pdftools::pdf_convert(paste0(dirhat,filename), dpi = 600)
#' This function reads a PNG from a file, then processes it
#' so that it is easier to read by OCR
#' @param image_filename The path of a file
#' @param rotate_param   A scalar parameter to rotate the image
#' output: a clean image
#'
fn_clean_image <- function(image_filename,rotate_param){
clean_image <- image_read(image_filename) %>%
image_resize("2000") %>%                  # RESIZE
image_convert(colorspace = 'gray') %>%    # CHANGE COLOR TO GRAY
image_trim() %>%                          # TRIM EDGES
image_transparent(color = "white", fuzz=75) %>%  # REMOVE BLACK SPECKS
image_background("white") %>%             # SET BACKGROUN COLOR TO WHITE
image_rotate(rotate_param)                # ROTATE IMAGE SO THAT IT IS
# EASIER TO RECOGNIZE TABLE
return(clean_image)
}
#' We use this to define the area of the image that contains the table (this
#' improves the quality of the OCR dramatically).
#' This function can also be uses to define specific boxes/cells within the
#' table.
#' @param image_object An image object, imported previously.
#' @param box_param    A list of 4 parameters specifying the coordinates
#'                     where the box starts, as well as the width and height.
#' output: A cropped image
#'
#'  Note: The box param arguments are based on the position by pixels
fn_crop_image <- function(image_object,box_param){
cropped_image <- image_object %>%
image_crop(geometry_area(width = box_param$width,
height = box_param$height,
box_param$hstart,   # Horizontal starting point
box_param$vstart))    # Vertical starting point
return(cropped_image)
}
#' This function conducts OCR over an image
#' @param image_object An image object, imported previously.
#' @param box_param    A list of parameters specifying the coordinates
#'                     where the box starts, as well as the width and height.
#' output: a dataframe with the information extracted, and a cropped image
#'
fn_extract_column <- function(image_object,box_param){
# Start by cropping the image
cropped_image <- fn_crop_image(image_object,box_param)
raw_text   <- cropped_image %>% image_ocr()
raw_tibble <- raw_text %>%
str_split(pattern = "\n") %>%
unlist() %>%
tibble(data = .)
if(nrow(raw_tibble) == 1){
raw_tibble <- raw_tibble %>% as.data.frame()
} else{
raw_tibble <- raw_tibble %>% filter(data != "") %>% as.data.frame()
}
return(list(raw_tibble = raw_tibble,
cropped_image = cropped_image))
}
# Define which page to crop
set_page <- 9
# Import configuration data for each page
config_data  <- read_excel(paste0(dirhat,"param/param_ocr.xlsx")) %>%
as.data.frame()
config_data
# Initial Crop
rotate_param       <- config_data[set_page,"rotate_param"]  # 0.75, stats for p1
image_param        <- list()
image_param$width  <- config_data[set_page,"width"]         # 1750
image_param$height <- config_data[set_page,"height"]        # 1950
image_param$hstart <- config_data[set_page,"hstart"]        # 100
image_param$vstart <- config_data[set_page,"vstart"]        # 230
clean_image        <- fn_clean_image(image_filename = pngfile_list[set_page],
rotate_param = rotate_param)
clean_image        <- fn_crop_image(image_object = clean_image,
box_param = image_param)
image_param
# IMPORTANT: Double check in the PDF to make sure that the table
# starts in the right place
# Some pages seem to have a higher rotation than others
clean_image
# Step 1
box_param    <- list()
box_param$width  = (image_param$width)*0.5
box_param$height = image_param$height
box_param$hstart = (image_param$width)*0.1
box_param$vstart = 0
box_param
results <- fn_extract_column(clean_image,box_param)
results$cropped_image
results
results <- fn_extract_column(clean_image,box_param)
results$cropped_image
list_organization_names = results$raw_tibble[,1]
total_rows = length(list_organization_names)
# Step 2: Create Data to fill
data <- tibble(box_value = rep(NA,total_rows),
index = rep(NA,total_rows),
name = list_organization_names)
# This is done to properly extract the names of organizations
# that are written across more than one line.
for(i in 1:total_rows){
print(i)
#i          <- 15
box_param        <- list()
box_param$width  <- (image_param$width)*0.07
box_param$height <- (image_param$height / total_rows)
box_param$hstart <- (image_param$width)*0.03
box_param$vstart <- (i-1)*(image_param$height / total_rows)
# Conduct OCR
results <- fn_extract_column(clean_image,box_param)
results$cropped_image
# Extract value from box
box_value   <- results$raw_tibble[1,1]
# Extract numeric value from box
regexp <- "[[:digit:]]+"      # This ensures that we only extract digits
value  <- as.numeric(str_extract(box_value, regexp))
if(i == 1){
value = value
} else{
if(box_value == ""){
value <- data[i-1,"index"]
} else {
value <- data[i-1,"index"] + 1
}
}
data[i,"box_value"] <- box_value
data[i,"index"]     <- value
}
# Process data to concatenate organization names that
# are written across multiple rows
data <- data %>% ungroup()
data <- data %>% group_by(index) %>%
summarise(box_value = paste(box_value,collapse = "|"),
name = paste(name,collapse = " ")) %>% ungroup()
data
# Step 4: Extract expiration date
box_param    <- list()
box_param$width  = (image_param$width)*0.2
box_param$height = image_param$height
box_param$hstart = (image_param$width)*0.6
box_param$vstart = 0
results <- fn_extract_column(clean_image,box_param)
results$cropped_image
list_mou_signing = results$raw_tibble[,1]
if( length(list_mou_signing) == nrow(data)){
data$list_mou_signing <- list_mou_signing
} else {
print("Number of rows of MOU does not match")
}
# Step 5: Extract expiration date
box_param    <- list()
box_param$width  = (image_param$width)*0.18
box_param$height = image_param$height
box_param$hstart = (image_param$width)*0.8
box_param$vstart = 0
results <- fn_extract_column(clean_image,box_param)
results$cropped_image
list_mou_validity = results$raw_tibble[,1]
if( length(list_mou_validity) == nrow(data)){
data$list_mou_validity <- list_mou_validity
} else {
print("Number of rows of MOU validity does not match")
}
data
