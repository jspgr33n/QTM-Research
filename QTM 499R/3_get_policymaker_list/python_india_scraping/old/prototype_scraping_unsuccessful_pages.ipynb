{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "killing-consent",
   "metadata": {},
   "source": [
    "prototype_scraping_unsuccessful_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-guest",
   "metadata": {},
   "source": [
    "# Script for scraping previously unsuccessful pages\n",
    "\n",
    "When we scraped all pages for a sector, some of the pages failed to scrape due to some random fluke. We store the URLs to these pages in a pickled list. In this notebook, we write code to scrape just these pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-lobby",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------DISPLAY SETTINGS FOR THE NOTEBOOK--------\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' #to see all outputs of a cell, as opposed to only the last one\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) #make the notebook take up more screen space (default is 60%)\n",
    "\n",
    "#-------FOR WORKING WITH DATA IN A DATAFRAME--------\n",
    "\n",
    "import pandas as pd #To store scraped data\n",
    "\n",
    "#-------SCRAPING SPECIFIC MODULES--------\n",
    "import requests #to conduct different forms of HTTP requests\n",
    "import html5lib #to construct tree structure of HTML data\n",
    "from bs4 import BeautifulSoup as soup #to parse the html data obtained from the scrape\n",
    "import time # to add wait time, to keep the website from kicking us out and also to let the page load before grabbing data\n",
    "from selenium import webdriver #to automate the navigating within the browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select #to select the features we want on the website via the scraper\n",
    "from selenium.webdriver.support.ui import WebDriverWait #again, to add wait times more 'implicitly'\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options #to use properties of the chrome webbrowser\n",
    "\n",
    "import random\n",
    "from joblib import Parallel, delayed # for parallelizing\n",
    "from tqdm import tqdm # this provides a visual progress tracker for loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-possession",
   "metadata": {},
   "source": [
    "## loading list of unsuccessful pages' URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsuccessful_pages = pd.read_csv('./unsuccessfully_scraped_pages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsuccessful_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-graphics",
   "metadata": {},
   "source": [
    "## setting up scraper for these pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.headless = True # True hides the navigating of the browser by the scraper, False shows you the tab/window opening and stuff getting clicked\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sector_urls(base_url): \n",
    "    driver.get(base_url)\n",
    "    sector_urls = [url.get_attribute('href') for url in driver.find_elements_by_class_name('bluelink11px')]\n",
    "    sector_urls = [url+'?per_page=100' for url in sector_urls]\n",
    "    return sector_urls\n",
    "\n",
    "\n",
    "def get_num_of_pages_for_sector(sector_num): \n",
    "    sector_url = sector_urls[sector_num]\n",
    "    driver.get(sector_url)\n",
    "    driver.find_elements_by_partial_link_text('Last')[0].click()\n",
    "    last_page_url = driver.current_url\n",
    "    total_pages = int(last_page_url.split('?')[0].rsplit('/',1)[1])\n",
    "    return total_pages\n",
    "\n",
    "def get_page_url(sector_url, page_num):\n",
    "    page_url = sector_url.split('?',1)[0][:-1]+f'{page_num}?per_page=100'\n",
    "    return page_url\n",
    "\n",
    "def scrape_a_single_page(page_url, unsuccessful_pages_df): \n",
    "    driver.get(page_url)\n",
    "    ngos_on_page = driver.find_elements_by_xpath(\"//a[contains(@onclick,'show_ngo_info')]\")\n",
    "    page_df = pd.DataFrame()\n",
    "    for i in range(0, len(ngos_on_page)): \n",
    "        print(f'scraping NGO number {i+1}')\n",
    "        ngo = ngos_on_page[i]\n",
    "        time.sleep(2)\n",
    "        ngo.click()\n",
    "#         print('Opened NGO info box')\n",
    "#         print('Extracting Details')\n",
    "        time.sleep(2)\n",
    "        name = driver.find_element_by_id('ngo_name_title').get_attribute('innerHTML')\n",
    "        uid = driver.find_element_by_id('UniqueID').get_attribute('innerHTML')\n",
    "        reg_with = driver.find_element_by_id('reg_with').get_attribute('innerHTML')\n",
    "        ngo_type = driver.find_element_by_id('ngo_type').get_attribute('innerHTML')\n",
    "        ngo_regno = driver.find_element_by_id('ngo_regno').get_attribute('innerHTML')\n",
    "        rc_upload = driver.find_element_by_id('rc_upload').get_attribute('innerHTML')\n",
    "        pc_upload = driver.find_element_by_id('pc_upload').get_attribute('innerHTML')\n",
    "        act_name = driver.find_element_by_id('ngo_act_name').get_attribute('innerHTML')\n",
    "        city_reg = driver.find_element_by_id('ngo_city_p').get_attribute('innerHTML')\n",
    "        state_reg = driver.find_element_by_id('ngo_state_p').get_attribute('innerHTML')\n",
    "        reg_date = driver.find_element_by_id('ngo_reg_date').get_attribute('innerHTML')\n",
    "        key_issues = driver.find_element_by_id('key_issues').get_attribute('innerHTML')\n",
    "        operational_states = driver.find_element_by_id('operational_states').get_attribute('innerHTML')\n",
    "        operational_districts = driver.find_element_by_id('operational_district').get_attribute('innerHTML')\n",
    "        fcra_details = driver.find_element_by_id('FCRA_details').get_attribute('innerHTML')\n",
    "        fcra_regno = driver.find_element_by_id('FCRA_reg_no').get_attribute('innerHTML')\n",
    "        details_achievement = driver.find_element_by_id('activities_achieve').get_attribute('innerHTML')\n",
    "        contact_address = driver.find_element_by_id('address').get_attribute('innerHTML')\n",
    "        contact_city = driver.find_element_by_id('city').get_attribute('innerHTML')\n",
    "        contact_state = driver.find_element_by_id('state_p_ngo').get_attribute('innerHTML')\n",
    "        contact_telephone = driver.find_element_by_id('phone_n').get_attribute('innerHTML')\n",
    "        contact_mobile = driver.find_element_by_id('mobile_n').get_attribute('innerHTML')\n",
    "        contact_website = driver.find_element_by_id('ngo_web_url').get_attribute('innerText')\n",
    "        contact_email = driver.find_element_by_id('email_n').get_attribute('innerHTML')\n",
    "#         print('Extracting details from members table...')\n",
    "        members_table = driver.find_element_by_id('member_table')\n",
    "        member_names =  [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[::4]]\n",
    "        member_designations = [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[1::4]]\n",
    "        member_pan = [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[2::4]]\n",
    "        member_aadhar = [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[3::4]]\n",
    "        member_name_designation_dict = dict(zip(member_names, member_designations))\n",
    "        member_name_pan_dict = dict(zip(member_names, member_pan))\n",
    "        member_name_aadhar_dict = dict(zip(member_names, member_aadhar))\n",
    "#         print('Extracting details from Source of Funds table...')\n",
    "        sof_table = driver.find_element_by_id('source_table')\n",
    "        dept_name = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[::5]]\n",
    "        source = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[1::5]]\n",
    "        financial_year = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[2::5]]\n",
    "        amount_sanctioned =[i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[3::5]]\n",
    "        purpose = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[4::5]]\n",
    "        year_amount_dict = dict(zip(financial_year, amount_sanctioned))\n",
    "        year_dept_dict = dict(zip(financial_year, dept_name))\n",
    "        year_source_dict = dict(zip(financial_year, source))\n",
    "        year_purpose_dict = dict(zip(financial_year, purpose))\n",
    "#         print('Storing extracted info into a dataframe')\n",
    "        df = pd.DataFrame()\n",
    "        df['ngo_name'] = [name]\n",
    "        df['unique_id'] = uid\n",
    "        df['registered_with'] = reg_with\n",
    "        df['type_of_ngo'] = ngo_type\n",
    "        df['registration_number'] = ngo_regno\n",
    "        df['copy_of_registration_certificate'] = rc_upload\n",
    "        df['copy_of_pan_card'] = pc_upload\n",
    "        df['act_name'] = act_name\n",
    "        df['city_of_registration'] = city_reg\n",
    "        df['state_of_registration'] = state_reg\n",
    "        df['registration_date'] = reg_date\n",
    "        df['key_issues'] = key_issues\n",
    "        df['operational_areas_states'] = operational_states\n",
    "        df['operational_areas_districts'] = operational_districts\n",
    "        df['FCRA_details'] = fcra_details\n",
    "        df['FCRA_registration_num'] = fcra_regno\n",
    "        df['details_of_achievement'] = details_achievement\n",
    "        df['contact_details_address'] = contact_address\n",
    "        df['contact_details_city'] = contact_city\n",
    "        df['contact_details_state'] = contact_state\n",
    "        df['contact_details_telephone'] = contact_telephone\n",
    "        df['contact_details_website'] = contact_website\n",
    "        df['contact_details_email'] = contact_email\n",
    "        df['members_names_designations'] = [member_name_designation_dict]\n",
    "        df['members_names_pan_availability'] = [member_name_pan_dict]\n",
    "        df['members_names_aadhar_availability'] = [member_name_aadhar_dict]\n",
    "        df['source_of_funds_amount_sanctioned'] = [year_amount_dict]\n",
    "        df['source_of_funds_department_name'] = [year_dept_dict]\n",
    "        df['source_of_funds_source'] = [year_source_dict]\n",
    "        df['source_of_funds_purpose'] = [year_purpose_dict]\n",
    "        df['page_url'] = page_url\n",
    "        df = df.merge(unsuccessful_pages_df, on='page_url', how='left')\n",
    "        df.drop('page_url', axis=1, inplace=True)\n",
    "#         print('Appending to dataframe for all NGOs...')\n",
    "        page_df = page_df.append(df)\n",
    "        page_df.to_csv(f'./sectorno_{sector_num}_pageno_{page_num}_ngo_scraping.csv', index=False)\n",
    "#         print('Closing pop-up window')\n",
    "        close_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#ngo_info_modal > div.modal-dialog.modal-lg > div > div.modal-header > button')))\n",
    "        time.sleep(2)\n",
    "        close_button.click()\n",
    "    return page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_scrape = df['page_url'].tolist()\n",
    "all_pages_df = pd.DataFrame()\n",
    "for url in tqdm(urls_to_scrape): \n",
    "    page_df = scrape_a_single_page(url)\n",
    "    all_pages_df = all_pages_df.append(page_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-disease",
   "metadata": {},
   "source": [
    "### code for removing curly brackets from the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('sectorno_0_pageno_10_ngo_scraping.csv') # replacing with whichever csv file has columns to remove curly brackets from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_curly_brackets(input_dict): \n",
    "    to_string = str(input_dict)\n",
    "    to_string= to_string.replace('{', '')\n",
    "    to_string= to_string.replace('}', '')\n",
    "    return to_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "member_cols = [x for x in test_df.columns if 'members' in x]\n",
    "funds_cols = [x for x in test_df.columns if 'source_of_funds' in x]\n",
    "dictionary_cols = member_cols + funds_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-civilian",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dictionary_cols: \n",
    "    test_df[col] = test_df[col].apply(lambda x: remove_curly_brackets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-provider",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
