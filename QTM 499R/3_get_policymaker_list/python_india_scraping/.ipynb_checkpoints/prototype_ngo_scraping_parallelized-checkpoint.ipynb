{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e156aa",
   "metadata": {},
   "source": [
    "prototype_ngo_scraping_parallelized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0859a8",
   "metadata": {},
   "source": [
    "# Prototype NGO Scraping: Scraping Information for Multiple NGOs\n",
    "The objective of this notebook is to play around with the NGO website scraping to in order to find a good way to scrape the list of NGO's. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e70e8",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25df34a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--------DISPLAY SETTINGS FOR THE NOTEBOOK--------\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all' #to see all outputs of a cell, as opposed to only the last one\n",
    "from IPython.display import display, HTML # Alejandro-Note: Changed from IPython.display\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) #make the notebook take up more screen space (default is 60%)\n",
    "\n",
    "#-------FOR WORKING WITH DATA IN A DATAFRAME--------\n",
    "\n",
    "import pandas as pd #To store scraped data\n",
    "\n",
    "#-------SCRAPING SPECIFIC MODULES--------\n",
    "import requests #to conduct different forms of HTTP requests\n",
    "import html5lib #to construct tree structure of HTML data\n",
    "from bs4 import BeautifulSoup as soup #to parse the html data obtained from the scrape\n",
    "import time # to add wait time, to keep the website from kicking us out and also to let the page load before grabbing data\n",
    "from selenium import webdriver #to automate the navigating within the browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select #to select the features we want on the website via the scraper\n",
    "from selenium.webdriver.support.ui import WebDriverWait #again, to add wait times more 'implicitly'\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options #to use properties of the chrome webbrowser\n",
    "from selenium.webdriver.remote.command import Command # Use to check whether the web driver is active\n",
    "\n",
    "import random\n",
    "from joblib import Parallel, delayed # for parallelizing\n",
    "from tqdm import tqdm # this provides a visual progress tracker for loops\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "import threading, queue\n",
    "from itertools import repeat\n",
    "from concurrent import futures\n",
    "from multiprocessing import Pool  # This is a CPU-based Pool\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import re # Use regular expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9d4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/GitHub/expert-forecasting/\n",
      "D:/GitHub/expert-forecasting/raw_data/3_get_policymaker_list/ngo_india_scraping/\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:/GitHub/expert-forecasting/\"\n",
    "store_directory = root_directory + \"raw_data/3_get_policymaker_list/ngo_india_scraping/\"\n",
    "\n",
    "print(root_directory)\n",
    "print(store_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cb744",
   "metadata": {},
   "source": [
    "### set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208e00a",
   "metadata": {},
   "source": [
    "#### specifying the url to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d865194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = 'https://ngodarpan.gov.in/index.php/home/sectorwise'\n",
    "base_url = 'https://ngodarpan.gov.in/index.php/home/statewise'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401b4b9",
   "metadata": {},
   "source": [
    "#### setting up selenium to use chrome as the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8768f7e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\asanc\\.wdm\\drivers\\chromedriver\\win32\\100.0.4896.60]\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True # True hides the navigating of the browser by the scraper, False shows you the tab/window opening and stuff getting clicked\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd42ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- This function creates a sector-specific URL\n",
    "def get_sector_urls(base_url): \n",
    "    driver.get(base_url)\n",
    "    sector_urls = [url.get_attribute('href') for url in driver.find_elements_by_class_name('bluelink11px')]\n",
    "    sector_urls = [url+'?per_page=100' for url in sector_urls]\n",
    "    return sector_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a024f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- This function captures the number of pages per sector\n",
    "def get_num_of_pages_for_sector(sector_num,driver): \n",
    "    sector_url = sector_urls[sector_num]\n",
    "    driver.get(sector_url)\n",
    "    list_buttonlast = driver.find_elements_by_partial_link_text('Last')\n",
    "    # If there are more than 10 pages, extract the link from the \"last page\".\n",
    "    # Otherwise count how many page buttons there are (excluding the \"previous\" and \"next\" buttons)\n",
    "    if len(list_buttonlast) > 0:\n",
    "        driver.find_elements_by_partial_link_text('Last')[0].click()\n",
    "        last_page_url = driver.current_url\n",
    "        total_pages = int(last_page_url.split('?')[0].rsplit('/',1)[1])\n",
    "    else:\n",
    "        list_page_nums = driver.find_elements_by_xpath(\"//ul[@class='pagination']//li\")\n",
    "        if len(list_page_nums) == 0:\n",
    "            total_pages = 1\n",
    "        else:\n",
    "            total_pages = len(list_page_nums)-2\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd58384c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_num_of_pages_for_sector(sector_num = 2,driver = driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c86dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- This function extracts the page url for a specific sector + page number\n",
    "def get_page_url(sector_url, page_num):\n",
    "    page_url = sector_url.split('?',1)[0][:-1]+f'{page_num}?per_page=100'\n",
    "    return page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c79bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- This function takes a list of NGO's per page, clicls through them sequentially,\n",
    "#--- and extracts information from pop-up.\n",
    "def scrape_a_single_page(sector_num, page_num): \n",
    "    \n",
    "    # The following code selects a web_driver from the dictionary.\n",
    "    # Both \"drivers_dict\" and \"max_workers are defined in the environment.\"\n",
    "    #driver = drivers_dict[page_num % max_workers_num]\n",
    "    \n",
    "    try:\n",
    "        thread_name= threading.current_thread().name\n",
    "        #print(threading.current_thread().name)\n",
    "        #sometime we are going to have different thread name in each iteration so a little regex might help\n",
    "        # This code avoids leaving additional threads running whenever \"ThreadPoolExecutor\" is run\n",
    "        thread_name = re.sub(\"ThreadPoolExecutor-(\\d*)_(\\d*)\", r\"ThreadPoolExecutor-0_\\2\", thread_name)\n",
    "        #print(f\"re.sub -> {thread_name}\")\n",
    "        driver = drivers_dict[thread_name]\n",
    "    except KeyError:\n",
    "        drivers_dict[threading.current_thread().name] = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "        driver = drivers_dict[threading.current_thread().name]    \n",
    "    \n",
    "    sector_url = sector_urls[sector_num]\n",
    "    page_url   = get_page_url(sector_url, page_num)\n",
    "    driver.get(page_url)\n",
    "    time.sleep(2)\n",
    "    ngos_on_page = driver.find_elements_by_xpath(\"//a[contains(@onclick,'show_ngo_info')]\")\n",
    "    page_df = pd.DataFrame()\n",
    "    for i in range(0, len(ngos_on_page)): \n",
    "    #for i in range(0, 3):\n",
    "        print(f'scraping NGO number {i+1}')\n",
    "        ngo = ngos_on_page[i]\n",
    "        time.sleep(2)\n",
    "        ngo.click()\n",
    "#         print('Opened NGO info box')\n",
    "#         print('Extracting Details')\n",
    "        time.sleep(2)\n",
    "        name = driver.find_element_by_id('ngo_name_title').get_attribute('innerHTML')\n",
    "        uid = driver.find_element_by_id('UniqueID').get_attribute('innerHTML')\n",
    "        reg_with = driver.find_element_by_id('reg_with').get_attribute('innerHTML')\n",
    "        ngo_type = driver.find_element_by_id('ngo_type').get_attribute('innerHTML')\n",
    "        ngo_regno = driver.find_element_by_id('ngo_regno').get_attribute('innerHTML')\n",
    "        rc_upload = driver.find_element_by_id('rc_upload').get_attribute('innerHTML')\n",
    "        pc_upload = driver.find_element_by_id('pc_upload').get_attribute('innerHTML')\n",
    "        act_name = driver.find_element_by_id('ngo_act_name').get_attribute('innerHTML')\n",
    "        city_reg = driver.find_element_by_id('ngo_city_p').get_attribute('innerHTML')\n",
    "        state_reg = driver.find_element_by_id('ngo_state_p').get_attribute('innerHTML')\n",
    "        reg_date = driver.find_element_by_id('ngo_reg_date').get_attribute('innerHTML')\n",
    "        key_issues = driver.find_element_by_id('key_issues').get_attribute('innerHTML')\n",
    "        operational_states = driver.find_element_by_id('operational_states').get_attribute('innerHTML')\n",
    "        operational_districts = driver.find_element_by_id('operational_district').get_attribute('innerHTML')\n",
    "        fcra_details = driver.find_element_by_id('FCRA_details').get_attribute('innerHTML')\n",
    "        fcra_regno = driver.find_element_by_id('FCRA_reg_no').get_attribute('innerHTML')\n",
    "        details_achievement = driver.find_element_by_id('activities_achieve').get_attribute('innerHTML')\n",
    "        contact_address = driver.find_element_by_id('address').get_attribute('innerHTML')\n",
    "        contact_city = driver.find_element_by_id('city').get_attribute('innerHTML')\n",
    "        contact_state = driver.find_element_by_id('state_p_ngo').get_attribute('innerHTML')\n",
    "        contact_telephone = driver.find_element_by_id('phone_n').get_attribute('innerHTML')\n",
    "        contact_mobile = driver.find_element_by_id('mobile_n').get_attribute('innerHTML')\n",
    "        contact_website = driver.find_element_by_id('ngo_web_url').get_attribute('innerText')\n",
    "        contact_email = driver.find_element_by_id('email_n').get_attribute('innerHTML')\n",
    "#         print('Extracting details from members table...')\n",
    "        members_table = driver.find_element_by_id('member_table')\n",
    "        member_names =  [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[::4]]\n",
    "        member_designations = [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[1::4]]\n",
    "        member_pan = [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[2::4]]\n",
    "        member_aadhar = [i.get_attribute('innerHTML') for i in members_table.find_elements_by_xpath('.//tr//td')[3::4]]\n",
    "        member_name_designation_dict = dict(zip(member_names, member_designations))\n",
    "        member_name_pan_dict = dict(zip(member_names, member_pan))\n",
    "        member_name_aadhar_dict = dict(zip(member_names, member_aadhar))\n",
    "#         print('Extracting details from Source of Funds table...')\n",
    "        sof_table = driver.find_element_by_id('source_table')\n",
    "        dept_name = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[::5]]\n",
    "        source = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[1::5]]\n",
    "        financial_year = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[2::5]]\n",
    "        amount_sanctioned =[i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[3::5]]\n",
    "        purpose = [i.get_attribute('innerHTML') for i in sof_table.find_elements_by_xpath('.//tr//td')[4::5]]\n",
    "        year_amount_dict = dict(zip(financial_year, amount_sanctioned))\n",
    "        year_dept_dict = dict(zip(financial_year, dept_name))\n",
    "        year_source_dict = dict(zip(financial_year, source))\n",
    "        year_purpose_dict = dict(zip(financial_year, purpose))\n",
    "#         print('Storing extracted info into a dataframe')\n",
    "        df = pd.DataFrame()\n",
    "        df['ngo_name'] = [name]\n",
    "        df['unique_id'] = uid\n",
    "        df['registered_with'] = reg_with\n",
    "        df['type_of_ngo'] = ngo_type\n",
    "        df['registration_number'] = ngo_regno\n",
    "        df['copy_of_registration_certificate'] = rc_upload\n",
    "        df['copy_of_pan_card'] = pc_upload\n",
    "        df['act_name'] = act_name\n",
    "        df['city_of_registration'] = city_reg\n",
    "        df['state_of_registration'] = state_reg\n",
    "        df['registration_date'] = reg_date\n",
    "        df['key_issues'] = key_issues\n",
    "        df['operational_areas_states'] = operational_states\n",
    "        df['operational_areas_districts'] = operational_districts\n",
    "        df['FCRA_details'] = fcra_details\n",
    "        df['FCRA_registration_num'] = fcra_regno\n",
    "        df['details_of_achievement'] = details_achievement\n",
    "        df['contact_details_address'] = contact_address\n",
    "        df['contact_details_city'] = contact_city\n",
    "        df['contact_details_state'] = contact_state\n",
    "        df['contact_details_telephone'] = contact_telephone\n",
    "        df['contact_details_website'] = contact_website\n",
    "        df['contact_details_email'] = contact_email\n",
    "        df['members_names_designations'] = [member_name_designation_dict]\n",
    "        df['members_names_pan_availability'] = [member_name_pan_dict]\n",
    "        df['members_names_aadhar_availability'] = [member_name_aadhar_dict]\n",
    "        df['source_of_funds_amount_sanctioned'] = [year_amount_dict]\n",
    "        df['source_of_funds_department_name'] = [year_dept_dict]\n",
    "        df['source_of_funds_source'] = [year_source_dict]\n",
    "        df['source_of_funds_purpose'] = [year_purpose_dict]\n",
    "        df['sector_number'] = sector_num\n",
    "        df['ngo_num'] = i\n",
    "        df['page_num'] = page_num\n",
    "        df['page_num_total'] = len(ngos_on_page)\n",
    "#         print('Appending to dataframe for all NGOs...')\n",
    "        page_df = pd.concat([page_df,df]) # Alejandro-note: Use pandas.concat instead. Old method deprecated. page_df.append(df) \n",
    "        page_df.to_csv(f'{store_directory}/sectorno_{sector_num}_pageno_{page_num}_ngo_scraping.csv', index=False)\n",
    "#         print('Closing pop-up window')\n",
    "        close_button = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#ngo_info_modal > div.modal-dialog.modal-lg > div > div.modal-header > button')))\n",
    "        time.sleep(2)\n",
    "        close_button.click()\n",
    "    # drivers_dict[threading.current_thread().name].quit()\n",
    "    return page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e99ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_a_sector(sector_num): \n",
    "    sector_url = sector_urls[sector_num]\n",
    "    total_pages = get_num_of_pages_for_sector(sector_num)\n",
    "    sector_df = pd.DataFrame()\n",
    "    unsuccessful_pages= pd.DataFrame()\n",
    "    for page in tqdm(range(1, total_pages+1)):\n",
    "        print(f'Scraping page number {page}')\n",
    "        page_url = get_page_url(sector_url, page)\n",
    "        print(f'Scraping {page_url}')\n",
    "        try: \n",
    "            page_df = scrape_a_single_page(sector_num, page_num= page)\n",
    "            sector_df = pd.concat([sector_df,page_df]) # Alejandro-note: Use pandas.concat instead. Old method deprecated. sector_df.append(page_df)\n",
    "            print(f'Page number {page} of {total_pages} finished')\n",
    "        except Exception as e: \n",
    "            print(f'Exception {e} occurred for sector number {0} and page number {page}' )\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df['sector_num'] = [sector_num]\n",
    "            temp_df['page_url'] =page_url\n",
    "            unsuccessful_pages = unsuccessful_pages.append(temp_df)\n",
    "            unsuccessful_pages.to_csv(f'{store_directory}/unsuccessfully_scraped_pages.csv', index=False)\n",
    "            continue\n",
    "    return sector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750e133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the list of ngos in each page\n",
    "def scrape_list_singleplage(sector_num,page_num,driver): \n",
    "    \n",
    "    sector_url = sector_urls[sector_num]\n",
    "    page_url   = get_page_url(sector_url, page_num)\n",
    "    driver.get(page_url)\n",
    "    time.sleep(2)\n",
    "    ngos_on_page = driver.find_elements_by_xpath(\"//a[contains(@onclick,'show_ngo_info')]\")\n",
    "    page_df = pd.DataFrame()\n",
    "\n",
    "    # This command extracts the table of ngos + some extraneous information\n",
    "    # The second row subsets the first 500 elements (100 organizations x 5 attributes)\n",
    "    ngolist_table = driver.find_element_by_xpath(\".//table[contains(@class,'table table-striped table-bordered table-hover Tax')]\");\n",
    "    raw_table = [i.get_attribute('innerHTML') for i in ngolist_table.find_elements_by_xpath('//tbody//tr//td')][0:500]\n",
    "\n",
    "    ngo_numinpage    = raw_table[0::5]\n",
    "    ngo_hyperlink    = raw_table[1::5]\n",
    "    ngo_registration = raw_table[2::5]\n",
    "    ngo_address      = raw_table[3::5]\n",
    "    ngo_sectors      = raw_table[4::5]\n",
    "    ngo_names = [i.split(\"<\")[1].split(\">\")[1] for i in ngo_hyperlink]\n",
    "\n",
    "    page_df['sector_num']    = [sector_num for i in range(0,len(ngo_numinpage))]\n",
    "    page_df['page_num']      = [page_num for i in range(0,len(ngo_numinpage))]\n",
    "    page_df['index_in_page'] = [i for i in range(0,len(ngo_numinpage))]\n",
    "    page_df['ngo_name']     = ngo_names\n",
    "    page_df['ngo_hyperlink'] = ngo_hyperlink\n",
    "    page_df['ngo_registration'] = ngo_registration\n",
    "    page_df['ngo_address']  = ngo_address\n",
    "    page_df['ngo_sectors']  = ngo_sectors\n",
    "\n",
    "    # page_df.to_csv(f'{store_directory}/list_sectorno_{sector_num}_page{page_num}_ngo_scraping.csv', index=False)    \n",
    "    \n",
    "    return(page_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa08961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_num = 1\n",
    "# try:\n",
    "#     thread_name= threading.current_thread().name\n",
    "#     #print(threading.current_thread().name)\n",
    "#     #sometime we are going to have different thread name in each iteration so a little regex might help\n",
    "#     # This code avoids leaving additional threads running whenever \"ThreadPoolExecutor\" is run\n",
    "#     thread_name = re.sub(\"ThreadPoolExecutor-(\\d*)_(\\d*)\", r\"ThreadPoolExecutor-0_\\2\", thread_name)\n",
    "#     #print(f\"re.sub -> {thread_name}\")\n",
    "#     driver = drivers_dict[thread_name]\n",
    "# except KeyError:\n",
    "#     drivers_dict[threading.current_thread().name] = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "#     driver = drivers_dict[threading.current_thread().name]  \n",
    "\n",
    "# sector_url = sector_urls[sector_num]\n",
    "# page_url   = get_page_url(sector_url, page_num)\n",
    "# driver.get(page_url)\n",
    "# time.sleep(2)\n",
    "# ngos_on_page = driver.find_elements_by_xpath(\"//a[contains(@onclick,'show_ngo_info')]\")\n",
    "# page_df = pd.DataFrame()\n",
    "\n",
    "# # This command extracts the table of ngos + some extraneous information\n",
    "# # The second row subsets the first 500 elements (100 organizations x 5 attributes)\n",
    "# ngolist_table = driver.find_element_by_xpath(\".//table[contains(@class,'table table-striped table-bordered table-hover Tax')]\");\n",
    "# raw_table = [i.get_attribute('innerHTML') for i in ngolist_table.find_elements_by_xpath('//tbody//tr//td')][0:500]\n",
    "\n",
    "# ngo_numinpage    = raw_table[0::5]\n",
    "# ngo_hyperlink    = raw_table[1::5]\n",
    "# ngo_registration = raw_table[2::5]\n",
    "# ngo_address      = raw_table[3::5]\n",
    "# ngo_sectors      = raw_table[4::5]\n",
    "# ngo_names = [i.split(\"<\")[1].split(\">\")[1] for i in ngo_hyperlink]\n",
    "\n",
    "# page_df['numinpage'] = ngo_numinpage\n",
    "# page_df['ngo_names'] = [sector_num for i in range(0,len(ngo_numinpage))]\n",
    "# page_df['ngo_names']   = ngo_names\n",
    "# page_df['ngo_hyperlink'] = ngo_hyperlink\n",
    "# page_df['ngo_registration'] = ngo_registration\n",
    "# page_df['ngo_address'] = ngo_address\n",
    "# page_df['ngo_sectors'] = ngo_sectors\n",
    "\n",
    "# # page_df\n",
    "\n",
    "# page_df.to_csv(f'{store_directory}/list_sectorno_{sector_num}_page{page_num}_ngo_scraping.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7274b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list_ngo_sector(sector_num): \n",
    "    print(f'Scraping sector number {sector_num}')  \n",
    "    sector_url = sector_urls[sector_num]\n",
    "    # print(f'Scraping sector number {sector_url}')\n",
    "    #total_pages = total_pages_list[sector_num]\n",
    "    #print(f'Scraping sector number {total_pages}')    \n",
    "    #print(f'Initiating Web Driver') \n",
    "    try:\n",
    "        thread_name= threading.current_thread().name\n",
    "        #print(threading.current_thread().name)\n",
    "        #sometime we are going to have different thread name in each iteration so a little regex might help\n",
    "        # This code avoids leaving additional threads running whenever \"ThreadPoolExecutor\" is run\n",
    "        thread_name = re.sub(\"ThreadPoolExecutor-(\\d*)_(\\d*)\", r\"ThreadPoolExecutor-0_\\2\", thread_name)\n",
    "        #print(f\"re.sub -> {thread_name}\")\n",
    "        driver = drivers_dict[thread_name]\n",
    "        print(f\"{thread_name}\")\n",
    "    except KeyError:\n",
    "        drivers_dict[threading.current_thread().name] = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "        driver = drivers_dict[threading.current_thread().name] \n",
    "    \n",
    "    print(f'Obtaining total number of pages')    \n",
    "    total_pages = get_num_of_pages_for_sector(sector_num,driver)\n",
    "    \n",
    "    time.sleep(4)\n",
    "    print(f'Initiating Scraping of NGO lists')\n",
    "    \n",
    "    sector_df = pd.DataFrame()\n",
    "    #unsuccessful_pages= pd.DataFrame()\n",
    "    for page in tqdm(range(1, total_pages+1)):\n",
    "        print(f'Scraping Sector {sector_num}, page number {page}')\n",
    "        page_url = get_page_url(sector_url, page)\n",
    "        #print(f'Scraping {page_url}')\n",
    "        page_df = scrape_list_singleplage(sector_num, page_num= page,driver=driver)\n",
    "        sector_df = pd.concat([sector_df,page_df])\n",
    "\n",
    "    sector_df.to_csv(f'{store_directory}/list_sectorno_{sector_num}_ngo_scraping.csv', index=False)    \n",
    "        \n",
    "    return sector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7bea6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining URLs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Obtaining URLs\")\n",
    "sector_urls = get_sector_urls(base_url)\n",
    "#total_pages_list = [get_num_of_pages_for_sector(sector_num) for sector_num in range(0,len(sector_urls))]\n",
    "drivers_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f02541b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sector_urls[9]\n",
    "# get_num_of_pages_for_sector(1,driver)\n",
    "\n",
    "# scrape_list_ngo_sector(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9713e605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(f\"Inititate Multithreading for Extracting List of NGOs\")\n",
    "# max_workers_num = 10\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#     result = executor.map(lambda sector_num_parallel: scrape_list_ngo_sector(sector_num_parallel), range(0,len(sector_urls)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035e5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sector_urls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad4d22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_pages(sector_num, start_page_num, end_page_num): \n",
    "    page_list = list(range(start_page_num, end_page_num+1))\n",
    "    main_df = pd.DataFrame()\n",
    "    unsuccessful_pages = pd.DataFrame()\n",
    "    sector_url = sector_urls[sector_num]\n",
    "    # The \"tqdm\" command helps us keep track of progress.\n",
    "    for page in tqdm(page_list): \n",
    "        print(f'Scraping page number {page}')\n",
    "        page_url = get_page_url(sector_url, page)\n",
    "        print(f'Scraping {page_url}')\n",
    "        try: \n",
    "            page_df = scrape_a_single_page(sector_num, page_num= page)\n",
    "            main_df = pd.concat([main_df,page_df]) # Alejandro-note: Use pandas.concat instead. Old method deprecated. main_df.append(page_df)\n",
    "        except Exception as e: \n",
    "            print(f'Exception {e} occurred for sector number {0} and page number {page}' )\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df['sector_num'] = [sector_num]\n",
    "            temp_df['page_url'] =page_url\n",
    "            unsuccessful_pages = pd.concat([unsuccessful_pages,temp_df]) # Alejandro-note: Use pandas.concat instead. Old method deprecated. unsuccessful_pages.append(temp_df) \n",
    "            unsuccessful_pages.to_csv('./unsuccessfully_scraped_pages_sector{sector_num}_page{page}.csv', index=False)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625dbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to compute the cumulative count of a dataset\n",
    "\n",
    "# list_allngos = pd.DataFrame()\n",
    "\n",
    "# #for sector_num in range(0,len(sector_urls)):\n",
    "# for sector_num in range(1,3):\n",
    "#     sector_df = pd.read_csv(f'{store_directory}/list_sectorno_{sector_num}_ngo_scraping.csv')\n",
    "#     list_allngos = pd.concat([list_allngos,sector_df])\n",
    "\n",
    "# # Create dianostic variables for possible duplicates in the list.\n",
    "# list_allngos['duplicates_index'] = list_allngos.groupby([\"ngo_hyperlink\",\"ngo_registration\"]).cumcount()+1\n",
    "# list_allngos['total_duplicates'] = list_allngos.groupby([\"ngo_hyperlink\",\"ngo_registration\"])[\"ngo_name\"].transform('size')\n",
    "\n",
    "# list_allngos.to_csv(f'{store_directory}/list_all_ngo_scraping.csv', index=False)    \n",
    "        \n",
    "# list_allngos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab91610b",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e0c4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sector_urls = get_sector_urls(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6242cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## This code creates initiates a list of web drivers used for multi-threading\n",
    "# ## Intuitively, this is like opening multiple tabs on Chrome web browser.\n",
    "\n",
    "# max_workers_num = 10\n",
    "# drivers_dict={}\n",
    "\n",
    "# for i in range(0,max_workers_num-1):\n",
    "#     drivers_dict[i] = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f39b9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scraping multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc6a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ngodarpan.gov.in/index.php/home/statewise_ngo/170/35/1?per_page=100'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sector_urls = get_sector_urls(base_url)\n",
    "max_workers_num = 30\n",
    "drivers_dict={}\n",
    "start_time = time.time()\n",
    "\n",
    "sector_num = 0\n",
    "sector_url = sector_urls[sector_num]\n",
    "sector_url\n",
    "driver.get(sector_url)\n",
    "\n",
    "get_num_of_pages_for_sector(sector_num,driver)\n",
    "#driver.find_elements_by_partial_link_text('Last')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "sector_urls = get_sector_urls(base_url)\n",
    "max_workers_num = 30\n",
    "drivers_dict={}\n",
    "start_time = time.time()\n",
    "\n",
    "sector_num = 29\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "sector_num = 30\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "sector_num = 31\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "sector_num = 32\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "sector_num = 33\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "sector_num = 34\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "sector_num = 35\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "sector_num = 36\n",
    "last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "      results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "        \n",
    "# sector_num = 26\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "# sector_num = 27\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "        \n",
    "# sector_num = 3\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "# sector_num = 4\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "# sector_num = 5\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "# sector_num = 6\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "# sector_num = 7\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "# sector_num = 8\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "# sector_num = 9\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "\n",
    "# sector_num = 10\n",
    "# last_sector_page = get_num_of_pages_for_sector(sector_num,driver)\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_num) as executor:\n",
    "#       results = executor.map(lambda i: scrape_multiple_pages(sector_num, i, i), range(1,last_sector_page+1))\n",
    "        \n",
    "        \n",
    "#Parallel(n_jobs=-2, verbose=200)(delayed(scrape_multiple_pages(sector_num, 1, last_sector_page)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d10227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code closes the webdrivers so that they don't occupy space in memory.\n",
    "drivers_dict\n",
    "\n",
    "#for i in range(0,len(drivers_dict)):\n",
    "#    drivers_dict[list(drivers_dict)[i]].quit();\n",
    "#     try:\n",
    "#         drivers_dict[i].execute(Command.STATUS)\n",
    "#         print(f\"Web Driver {i} stil active\")\n",
    "#     except:\n",
    "#         print(f\"Web Driver {i} successfully closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- The function took  %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45874922",
   "metadata": {},
   "source": [
    "Alternative: scraping all pages in a sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60037100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# sector_df = Parallel(n_jobs=50, verbose=200)(delayed(scrape_a_sector(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- The sector scraper took  %s seconds ---\" % (time.thttp://localhost:8888/notebooks/Documents/GitHub/expert-forecasting/code/3_get_policymaker_list/python_india_scraping/prototype_ngo_scraping_parallelized.ipynb#alternative:-scraping-all-pages-in-a-sectorime() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edbc00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e45c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d303e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
